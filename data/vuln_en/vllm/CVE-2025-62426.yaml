info:
  name: vllm
  cve: CVE-2025-62426
  summary: vLLM is vulnerable to Denial of Service (DoS) via large Chat Completion or Tokenization requests with specially crafted `chat_template_kwargs`.
  details: |
    The `/v1/chat/completions` and `/tokenize` endpoints in vLLM allow a `chat_template_kwargs` request parameter that is used in the code before it is properly validated against the chat template. This allows an attacker to craft `chat_template_kwargs` parameters that can block processing of the API server for long periods, delaying all other requests. Specifically, by setting `{"tokenize": True}` within `chat_template_kwargs`, tokenization, a blocking operation, can be forced to occur as part of applying the chat template, leading to a DoS.
  cvss: CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H
  severity: HIGH
  security_advise: |
    1. Upgrade to vLLM version 0.11.1 or later.
    2. Implement strict validation of `chat_template_kwargs` to prevent unexpected parameter overrides.
    3. Consider not passing `chat_template_kwargs` as unpacked kwargs directly, but instead as a dictionary, and only unpack them after validation logic.
rule: version >= "0.5.5" && version < "0.11.1"
references:
  - https://github.com/vllm-project/vllm/security/advisories/GHSA-69j4-grxj-j64p
  - https://nvd.nist.gov/vuln/detail/CVE-2025-62426
  - https://github.com/vllm-project/vllm/pull/27205
  - https://github.com/vllm-project/vllm/commit/3ada34f9cb4d1af763fdfa3b481862a93eb6bd2b
  - https://github.com/vllm-project/vllm/blob/2a6dc67eb520ddb9c4138d8b35ed6fe6226997fb/vllm/entrypoints/chat_utils.py#L1602-L1610
  - https://github.com/vllm-project/vllm/blob/2a6dc67eb520ddb9c4138d8b35ed6fe6226997fb/vllm/entrypoints/openai/serving_engine.py#L809-L814