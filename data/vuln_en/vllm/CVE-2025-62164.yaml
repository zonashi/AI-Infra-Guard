info:
  name: vllm
  cve: CVE-2025-62164
  summary: vLLM deserialization vulnerability leading to Denial of Service and potential Remote Code Execution.
  details: |
    A memory corruption vulnerability exists in vLLM versions 0.10.2 and later, specifically in the Completions API endpoint.
    When processing user-supplied prompt embeddings, the endpoint loads serialized tensors using `torch.load()` without sufficient validation.
    Due to a change in PyTorch 2.8.0, sparse tensor integrity checks are disabled by default, allowing maliciously crafted tensors to bypass internal bounds checks.
    This can trigger an out-of-bounds memory write during the `to_dense()` call, leading to a crash (Denial of Service) and potentially remote code execution (RCE) on the server hosting vLLM.
  cvss: CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H
  severity: HIGH
  security_advise: |
    1. Upgrade vLLM to version 0.11.1 or later.
    2. Implement explicit validity checks for sparse tensors when loading user-provided tensors, potentially using `torch.sparse.check_sparse_tensor_invariants` context manager.
  references:
    - https://github.com/vllm-project/vllm/security/advisories/GHSA-mrw7-hf4f-83pf
    - https://nvd.nist.gov/vuln/detail/CVE-2025-62164
    - https://github.com/vllm-project/vllm/pull/27204
    - https://github.com/vllm-project/vllm/commit/58fab50d82838d5014f4a14d991fdb9352c9c84b
    - https://github.com/vllm-project/vllm
rule: version >= "0.10.2" && version < "0.11.1"