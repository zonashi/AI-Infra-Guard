info:
  name: vllm
  cve: CVE-2025-62426
  summary: vLLM 易受拒绝服务 (DoS) 攻击，通过特制 `chat_template_kwargs` 的大型聊天完成或分词请求。
  details: |
    vLLM 中的 `/v1/chat/completions` 和 `/tokenize` 端点允许一个 `chat_template_kwargs` 请求参数，该参数在代码中未经正确验证就应用于聊天模板。这使得攻击者可以制作 `chat_template_kwargs` 参数，这些参数可以长时间阻塞 API 服务器的处理，从而延迟所有其他请求。具体来说，通过在 `chat_template_kwargs` 中设置 `{"tokenize": True}`，可以强制将分词（一个阻塞操作）作为应用聊天模板的一部分进行，从而导致拒绝服务 (DoS)。
  cvss: CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H
  severity: HIGH
  security_advise: |
    1. 升级到 vLLM 0.11.1 或更高版本。
    2. 对 `chat_template_kwargs` 实施严格验证，以防止意外的参数覆盖。
    3. 考虑不要将 `chat_template_kwargs` 作为解包的 kwargs 直接传递，而是作为字典传递，并且仅在验证逻辑之后再解包。
rule: version >= "0.5.5" && version < "0.11.1"
references:
  - https://github.com/vllm-project/vllm/security/advisories/GHSA-69j4-grxj-j64p
  - https://nvd.nist.gov/vuln/detail/CVE-2025-62426
  - https://github.com/vllm-project/vllm/pull/27205
  - https://github.com/vllm-project/vllm/commit/3ada34f9cb4d1af763fdfa3b481862a93eb6bd2b
  - https://github.com/vllm-project/vllm/blob/2a6dc67eb520ddb9c4138d8b35ed6fe6226997fb/vllm/entrypoints/chat_utils.py#L1602-L1610
  - https://github.com/vllm-project/vllm/blob/2a6dc67eb520ddb9c4138d8b35ed6fe6226997fb/vllm/entrypoints/openai/serving_engine.py#L809-L814